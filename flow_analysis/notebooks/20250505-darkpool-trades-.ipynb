{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0147ea-0b3f-4bb8-9479-002746ebea11",
   "metadata": {},
   "source": [
    "# Get all Historical Trades where DTE > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ffd448-86ea-463b-b21b-8571d93d44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ALL historical dark pool trades where DTE > 0...\n",
      "This might take some time depending on the database size...\n",
      "\n",
      "Fetching and saving darkpool trades...\n",
      "Executing query...\n",
      "Fetched 10000 rows so far... (5019.17 rows/sec, chunk 1)\n",
      "Fetched 16062 rows so far... (6534.12 rows/sec, chunk 2)\n",
      "Writing final chunks to data/darkpool_trades_historical_dte_gt0_20250506_101732.csv...\n",
      "Fetch completed in 2.91 seconds\n",
      "Completed saving 16062 darkpool trades to data/darkpool_trades_historical_dte_gt0_20250506_101732.csv\n",
      "\n",
      "Fetching and saving options flow data...\n",
      "Executing query...\n",
      "Fetch completed in 0.27 seconds\n",
      "Completed saving 0 option flows to data/options_flow_historical_dte_gt0_20250506_101732.csv\n",
      "\n",
      "Full data fetch complete.\n",
      "\n",
      "Generating summary statistics...\n",
      "Error generating summary statistics: [Errno 2] No such file or directory: 'data/options_flow_historical_dte_gt0_20250506_101732.csv'\n",
      "Data has been saved to files, but summary statistics could not be generated.\n",
      "\n",
      "Full data is available in:\n",
      "- data/darkpool_trades_historical_dte_gt0_20250506_101732.csv\n",
      "- data/options_flow_historical_dte_gt0_20250506_101732.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Database connection setup\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'defaultdb',\n",
    "    'user': 'doadmin',\n",
    "    'password': 'AVNS_SrG4Bo3B7uCNEPONkE4', # Consider moving to environment variable\n",
    "    'host': 'vvv-trading-db-do-user-2110609-0.i.db.ondigitalocean.com',\n",
    "    'port': '25060'\n",
    "}\n",
    "\n",
    "DATABASE_URL = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "engine = create_engine(DATABASE_URL, connect_args={'sslmode': 'require'})\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Generate filenames with current timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "darkpool_filename = f'data/darkpool_trades_historical_dte_gt0_{timestamp}.csv'\n",
    "options_filename = f'data/options_flow_historical_dte_gt0_{timestamp}.csv'\n",
    "\n",
    "print(\"Fetching ALL historical dark pool trades where DTE > 0...\")\n",
    "print(\"This might take some time depending on the database size...\")\n",
    "\n",
    "# Modified query for dark pool trades - fetch all historical\n",
    "darkpool_query = \"\"\"\n",
    "SELECT \n",
    "    t.*,\n",
    "    date_trunc('hour', t.executed_at) as trade_hour,\n",
    "    t.price - t.nbbo_bid as price_impact,\n",
    "    (t.price - t.nbbo_bid) / t.nbbo_bid as price_impact_pct,\n",
    "    CASE \n",
    "        WHEN t.size >= 10000 THEN 'Block Trade'\n",
    "        WHEN t.premium >= 0.02 THEN 'High Premium'\n",
    "        ELSE 'Regular'\n",
    "    END as trade_type,\n",
    "    count(*) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as trades_per_hour,\n",
    "    sum(t.size) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as volume_per_hour\n",
    "FROM trading.darkpool_trades t\n",
    "ORDER BY t.executed_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# Modified query for options flow - fetch all historical\n",
    "options_query = \"\"\"\n",
    "SELECT \n",
    "    f.*,\n",
    "    date_trunc('hour', f.collected_at) as flow_hour,\n",
    "    CASE \n",
    "        WHEN f.premium >= 1000000 THEN 'Whale'\n",
    "        WHEN f.premium >= 100000 THEN 'Large'\n",
    "        ELSE 'Regular'\n",
    "    END as flow_size,\n",
    "    count(*) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as flows_per_hour,\n",
    "    sum(f.premium) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as premium_per_hour,\n",
    "    sum(f.contract_size) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as contracts_per_hour\n",
    "FROM trading.options_flow f\n",
    "ORDER BY f.collected_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# Process data in chunks to handle potentially large datasets\n",
    "def fetch_in_chunks(query, engine, filename, chunk_size=10000):\n",
    "    \"\"\"Fetch data in chunks to avoid memory issues with large datasets.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query as string\n",
    "        engine: SQLAlchemy engine\n",
    "        filename: Output file to save chunks\n",
    "        chunk_size: Number of rows per chunk\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    connection = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"Executing query...\")\n",
    "        # Convert the string query to a SQLAlchemy text object\n",
    "        result = connection.execute(text(query))\n",
    "        \n",
    "        total_rows = 0\n",
    "        chunk_num = 0\n",
    "        \n",
    "        while True:\n",
    "            chunk = result.fetchmany(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "                \n",
    "            chunk_df = pd.DataFrame(chunk, columns=result.keys())\n",
    "            chunks.append(chunk_df)\n",
    "            total_rows += len(chunk_df)\n",
    "            chunk_num += 1\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            rows_per_sec = total_rows / elapsed if elapsed > 0 else 0\n",
    "            \n",
    "            print(f\"Fetched {total_rows} rows so far... ({rows_per_sec:.2f} rows/sec, chunk {chunk_num})\")\n",
    "            \n",
    "            # Write chunk to disk to avoid memory issues\n",
    "            if len(chunks) >= 5:  # After accumulating 5 chunks\n",
    "                print(f\"Writing chunks to {filename}...\")\n",
    "                combined = pd.concat(chunks, ignore_index=True)\n",
    "                if not os.path.exists(filename):\n",
    "                    combined.to_csv(filename, index=False)\n",
    "                else:\n",
    "                    combined.to_csv(filename, mode='a', header=False, index=False)\n",
    "                chunks = []  # Clear the chunks from memory\n",
    "                print(f\"Wrote {len(combined)} rows to file, continuing fetch...\")\n",
    "                \n",
    "        # Process any remaining chunks\n",
    "        if chunks:\n",
    "            print(f\"Writing final chunks to {filename}...\")\n",
    "            combined = pd.concat(chunks, ignore_index=True)\n",
    "            if not os.path.exists(filename):\n",
    "                combined.to_csv(filename, index=False)\n",
    "            else:\n",
    "                combined.to_csv(filename, mode='a', header=False, index=False)\n",
    "                \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Fetch completed in {total_time:.2f} seconds\")\n",
    "        return total_rows\n",
    "        \n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "# Fetch and save darkpool trades in chunks\n",
    "print(\"\\nFetching and saving darkpool trades...\")\n",
    "total_darkpool_rows = fetch_in_chunks(darkpool_query, engine, darkpool_filename)\n",
    "print(f\"Completed saving {total_darkpool_rows} darkpool trades to {darkpool_filename}\")\n",
    "\n",
    "# Fetch and save options flow data in chunks\n",
    "print(\"\\nFetching and saving options flow data...\")\n",
    "total_options_rows = fetch_in_chunks(options_query, engine, options_filename)\n",
    "print(f\"Completed saving {total_options_rows} option flows to {options_filename}\")\n",
    "\n",
    "print(\"\\nFull data fetch complete.\")\n",
    "print(\"\\nGenerating summary statistics...\")\n",
    "\n",
    "# Process and summarize data from the saved files\n",
    "try:\n",
    "    # Load a sample of the data to generate summary statistics\n",
    "    trades_sample = pd.read_csv(darkpool_filename, nrows=100000)\n",
    "    options_sample = pd.read_csv(options_filename, nrows=100000)\n",
    "    \n",
    "    # Process darkpool trades\n",
    "    trades_sample['executed_at'] = pd.to_datetime(trades_sample['executed_at'])\n",
    "    if 'collection_time' in trades_sample.columns:\n",
    "        trades_sample['collection_time'] = pd.to_datetime(trades_sample['collection_time'])\n",
    "    trades_sample['trade_hour'] = pd.to_datetime(trades_sample['trade_hour'])\n",
    "    \n",
    "    # Process options flow\n",
    "    options_sample['collected_at'] = pd.to_datetime(options_sample['collected_at'])\n",
    "    if 'created_at' in options_sample.columns:\n",
    "        options_sample['created_at'] = pd.to_datetime(options_sample['created_at'])\n",
    "    options_sample['expiry'] = pd.to_datetime(options_sample['expiry'])\n",
    "    options_sample['flow_hour'] = pd.to_datetime(options_sample['flow_hour'])\n",
    "    \n",
    "    # Print darkpool trade summary (from sample)\n",
    "    print(\"\\nDarkpool Trade sample summary by symbol (first 100k rows):\")\n",
    "    print(trades_sample.groupby('symbol').agg({\n",
    "        'size': ['count', 'sum', 'mean'],\n",
    "        'premium': ['mean', 'max'],\n",
    "        'price_impact_pct': 'mean'\n",
    "    }).round(2))\n",
    "    \n",
    "    # Print options flow summary (from sample)\n",
    "    print(\"\\nOptions Flow sample summary by symbol (first 100k rows):\")\n",
    "    print(options_sample.groupby('symbol').agg({\n",
    "        'premium': ['count', 'sum', 'mean', 'max'],\n",
    "        'contract_size': ['sum', 'mean'],\n",
    "        'iv_rank': 'mean'\n",
    "    }).round(2))\n",
    "    \n",
    "    # Print date ranges for samples\n",
    "    print(\"\\nDate ranges (from samples):\")\n",
    "    print(\"Darkpool Trades:\")\n",
    "    print(f\"Earliest trade in sample: {trades_sample['executed_at'].min()}\")\n",
    "    print(f\"Latest trade in sample: {trades_sample['executed_at'].max()}\")\n",
    "    print(f\"Total trades fetched: {total_darkpool_rows}\")\n",
    "    \n",
    "    print(\"\\nOptions Flow:\")\n",
    "    print(f\"Earliest flow in sample: {options_sample['collected_at'].min()}\")\n",
    "    print(f\"Latest flow in sample: {options_sample['collected_at'].max()}\")\n",
    "    print(f\"Total flows fetched: {total_options_rows}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating summary statistics: {str(e)}\")\n",
    "    print(\"Data has been saved to files, but summary statistics could not be generated.\")\n",
    "\n",
    "print(\"\\nFull data is available in:\")\n",
    "print(f\"- {darkpool_filename}\")\n",
    "print(f\"- {options_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d12078-df8f-4de5-b03c-756c5b361ef2",
   "metadata": {},
   "source": [
    "# Get Trades for Last 24 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a723e1a6-3e0c-4252-8294-28fcd3b1335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dark pool trades from last 24 hours...\n",
      "Fetching options flow data from last 24 hours...\n",
      "\n",
      "Saved 624 trades to data/darkpool_trades_24h_20250505_220016.csv\n",
      "Saved 0 option flows to data/options_flow_24h_20250505_220016.csv\n",
      "\n",
      "Darkpool Trade summary by symbol:\n",
      "        size                           premium              price_impact_pct\n",
      "       count       sum       mean         mean          max             mean\n",
      "symbol                                                                      \n",
      "AAPL      17   15143.0     890.76    178183.42    300217.50              0.0\n",
      "ABNB       4    6100.0    1525.00    190850.87    200291.84              0.0\n",
      "ALL        1     600.0     600.00    120820.02    120820.02             -0.0\n",
      "AMAT       2    1382.0     691.00    107569.09    110260.95              0.0\n",
      "AMD        2    5700.0    2850.00    288955.84    466424.82              0.0\n",
      "...      ...       ...        ...          ...          ...              ...\n",
      "XLK        1  101326.0  101326.00  21954436.14  21954436.14              0.0\n",
      "XLU        5   27450.0    5490.00    436870.87    866735.10              0.0\n",
      "XLY        1     900.0     900.00    179774.82    179774.82              0.0\n",
      "XOM        1    2500.0    2500.00    258278.00    258278.00             -0.0\n",
      "ZBH        3    5438.0    1812.67    163715.19    189346.52              0.0\n",
      "\n",
      "[173 rows x 6 columns]\n",
      "\n",
      "Options Flow summary by symbol:\n",
      "Empty DataFrame\n",
      "Columns: [(premium, count), (premium, sum), (premium, mean), (premium, max), (contract_size, sum), (contract_size, mean), (iv_rank, mean)]\n",
      "Index: []\n",
      "\n",
      "Date ranges:\n",
      "Darkpool Trades:\n",
      "Earliest trade: 2025-05-05 19:05:00+00:00\n",
      "Latest trade: 2025-05-05 20:00:00+00:00\n",
      "Total trades: 624\n",
      "Total volume: 3,654,616\n",
      "\n",
      "Options Flow:\n",
      "Earliest flow: NaT\n",
      "Latest flow: NaT\n",
      "Total flows: 0\n",
      "Total premium: $0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Database connection setup (same as before)\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'defaultdb',\n",
    "    'user': 'doadmin',\n",
    "    'password': 'AVNS_SrG4Bo3B7uCNEPONkE4',\n",
    "    'host': 'vvv-trading-db-do-user-2110609-0.i.db.ondigitalocean.com',\n",
    "    'port': '25060'\n",
    "}\n",
    "\n",
    "DATABASE_URL = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "engine = create_engine(DATABASE_URL, connect_args={'sslmode': 'require'})\n",
    "\n",
    "# Calculate timestamp for 24 hours ago\n",
    "twenty_four_hours_ago = datetime.now() - timedelta(hours=24)\n",
    "\n",
    "# Query for dark pool trades from last 24 hours\n",
    "darkpool_query = \"\"\"\n",
    "SELECT \n",
    "    t.*,\n",
    "    date_trunc('hour', t.executed_at) as trade_hour,\n",
    "    t.price - t.nbbo_bid as price_impact,\n",
    "    (t.price - t.nbbo_bid) / t.nbbo_bid as price_impact_pct,\n",
    "    CASE \n",
    "        WHEN t.size >= 10000 THEN 'Block Trade'\n",
    "        WHEN t.premium >= 0.02 THEN 'High Premium'\n",
    "        ELSE 'Regular'\n",
    "    END as trade_type,\n",
    "    count(*) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as trades_per_hour,\n",
    "    sum(t.size) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as volume_per_hour\n",
    "FROM trading.darkpool_trades t\n",
    "WHERE t.executed_at >= %(twenty_four_hours_ago)s\n",
    "ORDER BY t.executed_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# Query for options flow from last 24 hours\n",
    "options_query = \"\"\"\n",
    "SELECT \n",
    "    f.*,\n",
    "    date_trunc('hour', f.collected_at) as flow_hour,\n",
    "    CASE \n",
    "        WHEN f.premium >= 1000000 THEN 'Whale'\n",
    "        WHEN f.premium >= 100000 THEN 'Large'\n",
    "        ELSE 'Regular'\n",
    "    END as flow_size,\n",
    "    count(*) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as flows_per_hour,\n",
    "    sum(f.premium) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as premium_per_hour,\n",
    "    sum(f.contract_size) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as contracts_per_hour\n",
    "FROM trading.options_flow f\n",
    "WHERE f.collected_at >= %(twenty_four_hours_ago)s\n",
    "ORDER BY f.collected_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# Fetch both datasets with the time parameter\n",
    "print(\"Fetching dark pool trades from last 24 hours...\")\n",
    "trades_df = pd.read_sql_query(darkpool_query, engine, params={'twenty_four_hours_ago': twenty_four_hours_ago})\n",
    "\n",
    "print(\"Fetching options flow data from last 24 hours...\")\n",
    "options_df = pd.read_sql_query(options_query, engine, params={'twenty_four_hours_ago': twenty_four_hours_ago})\n",
    "\n",
    "# Process darkpool trades\n",
    "trades_df['executed_at'] = pd.to_datetime(trades_df['executed_at'])\n",
    "trades_df['collection_time'] = pd.to_datetime(trades_df['collection_time'])\n",
    "trades_df['trade_hour'] = pd.to_datetime(trades_df['trade_hour'])\n",
    "\n",
    "# Process options flow\n",
    "options_df['collected_at'] = pd.to_datetime(options_df['collected_at'])\n",
    "options_df['created_at'] = pd.to_datetime(options_df['created_at'])\n",
    "options_df['expiry'] = pd.to_datetime(options_df['expiry'])\n",
    "options_df['flow_hour'] = pd.to_datetime(options_df['flow_hour'])\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Generate filenames with current timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "darkpool_filename = f'data/darkpool_trades_24h_{timestamp}.csv'\n",
    "options_filename = f'data/options_flow_24h_{timestamp}.csv'\n",
    "\n",
    "# Save both datasets\n",
    "trades_df.to_csv(darkpool_filename, index=False)\n",
    "options_df.to_csv(options_filename, index=False)\n",
    "\n",
    "print(f\"\\nSaved {len(trades_df)} trades to {darkpool_filename}\")\n",
    "print(f\"Saved {len(options_df)} option flows to {options_filename}\")\n",
    "\n",
    "# Print darkpool trade summary\n",
    "print(\"\\nDarkpool Trade summary by symbol:\")\n",
    "print(trades_df.groupby('symbol').agg({\n",
    "    'size': ['count', 'sum', 'mean'],\n",
    "    'premium': ['mean', 'max'],\n",
    "    'price_impact_pct': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "# Print options flow summary\n",
    "print(\"\\nOptions Flow summary by symbol:\")\n",
    "print(options_df.groupby('symbol').agg({\n",
    "    'premium': ['count', 'sum', 'mean', 'max'],\n",
    "    'contract_size': ['sum', 'mean'],\n",
    "    'iv_rank': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "# Print date ranges for both datasets\n",
    "print(\"\\nDate ranges:\")\n",
    "print(\"Darkpool Trades:\")\n",
    "print(f\"Earliest trade: {trades_df['executed_at'].min()}\")\n",
    "print(f\"Latest trade: {trades_df['executed_at'].max()}\")\n",
    "print(f\"Total trades: {len(trades_df)}\")\n",
    "print(f\"Total volume: {trades_df['size'].sum():,.0f}\")\n",
    "\n",
    "print(\"\\nOptions Flow:\")\n",
    "print(f\"Earliest flow: {options_df['collected_at'].min()}\")\n",
    "print(f\"Latest flow: {options_df['collected_at'].max()}\")\n",
    "print(f\"Total flows: {len(options_df)}\")\n",
    "print(f\"Total premium: ${options_df['premium'].sum():,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
