{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0147ea-0b3f-4bb8-9479-002746ebea11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get all Historical Trades where DTE > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57ffd448-86ea-463b-b21b-8571d93d44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ALL historical dark pool trades...\n",
      "This might take some time depending on the database size...\n",
      "\n",
      "Fetching and saving darkpool trades...\n",
      "Executing query...\n",
      "Fetched 10000 rows so far... (5023.94 rows/sec, chunk 1)\n",
      "Fetched 20000 rows so far... (8349.21 rows/sec, chunk 2)\n",
      "Fetched 30000 rows so far... (11016.54 rows/sec, chunk 3)\n",
      "Fetched 40000 rows so far... (13205.39 rows/sec, chunk 4)\n",
      "Fetched 46704 rows so far... (14101.23 rows/sec, chunk 5)\n",
      "Writing chunks to data/darkpool_trades_historical_20250509_160137.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/kjq9z2x165b9g6jc_0l_ljz80000gn/T/ipykernel_1925/2048889616.py:101: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat(chunks, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 46704 rows to file, continuing fetch...\n",
      "Fetch completed in 4.64 seconds\n",
      "Completed saving 46704 darkpool trades to data/darkpool_trades_historical_20250509_160137.csv\n",
      "\n",
      "Fetching and saving options flow data...\n",
      "Executing query...\n",
      "Fetch completed in 0.25 seconds\n",
      "Completed saving 0 option flows to data/options_flow_historical_20250509_160137.csv\n",
      "\n",
      "Full data fetch complete.\n",
      "\n",
      "Generating summary statistics...\n",
      "Error generating summary statistics: [Errno 2] No such file or directory: 'data/options_flow_historical_20250509_160137.csv'\n",
      "Data has been saved to files, but summary statistics could not be generated.\n",
      "\n",
      "Full data is available in:\n",
      "- data/darkpool_trades_historical_20250509_160137.csv\n",
      "- data/options_flow_historical_20250509_160137.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/kjq9z2x165b9g6jc_0l_ljz80000gn/T/ipykernel_1925/2048889616.py:147: DtypeWarning: Columns (16,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trades_sample = pd.read_csv(darkpool_filename, nrows=100000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Database connection setup\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'defaultdb',\n",
    "    'user': 'doadmin',\n",
    "    'password': 'AVNS_SrG4Bo3B7uCNEPONkE4',\n",
    "    'host': 'vvv-trading-db-do-user-2110609-0.i.db.ondigitalocean.com',\n",
    "    'port': '25060'\n",
    "}\n",
    "\n",
    "DATABASE_URL = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "engine = create_engine(DATABASE_URL, connect_args={'sslmode': 'require'})\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Generate filenames with current timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "darkpool_filename = f'data/darkpool_trades_historical_{timestamp}.csv'\n",
    "options_filename = f'data/options_flow_historical_{timestamp}.csv'\n",
    "\n",
    "print(\"Fetching ALL historical dark pool trades...\")\n",
    "print(\"This might take some time depending on the database size...\")\n",
    "\n",
    "# Modified query for dark pool trades - fetch all historical\n",
    "darkpool_query = \"\"\"\n",
    "SELECT \n",
    "    t.*,\n",
    "    date_trunc('hour', t.executed_at) as trade_hour,\n",
    "    t.price - t.nbbo_bid as price_impact,\n",
    "    CASE \n",
    "        WHEN t.nbbo_bid IS NULL OR t.nbbo_bid = 0 THEN NULL\n",
    "        ELSE (t.price - t.nbbo_bid) / t.nbbo_bid\n",
    "    END as price_impact_pct,\n",
    "    CASE \n",
    "        WHEN t.size >= 10000 THEN 'Block Trade'\n",
    "        WHEN t.premium >= 1000000 THEN 'High Premium'\n",
    "        ELSE 'Regular'\n",
    "    END as trade_type,\n",
    "    count(*) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as trades_per_hour,\n",
    "    sum(t.size) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as volume_per_hour\n",
    "FROM trading.darkpool_trades t\n",
    "ORDER BY t.executed_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# Modified query for options flow - fetch all historical\n",
    "options_query = \"\"\"\n",
    "SELECT \n",
    "    f.*,\n",
    "    date_trunc('hour', f.collected_at) as flow_hour,\n",
    "    CASE \n",
    "        WHEN f.premium >= 1000000 THEN 'Whale'\n",
    "        WHEN f.premium >= 100000 THEN 'Large'\n",
    "        ELSE 'Regular'\n",
    "    END as flow_size,\n",
    "    count(*) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as flows_per_hour,\n",
    "    sum(f.premium) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as premium_per_hour,\n",
    "    sum(f.contract_size) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as contracts_per_hour\n",
    "FROM trading.options_flow f\n",
    "ORDER BY f.collected_at DESC\n",
    "\"\"\"\n",
    "\n",
    "def fetch_in_chunks(query, engine, filename, chunk_size=10000):\n",
    "    \"\"\"Fetch data in chunks to avoid memory issues with large datasets.\"\"\"\n",
    "    start_time = time.time()\n",
    "    connection = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"Executing query...\")\n",
    "        result = connection.execute(text(query))\n",
    "        \n",
    "        total_rows = 0\n",
    "        chunk_num = 0\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                chunk = result.fetchmany(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                    \n",
    "                chunk_df = pd.DataFrame(chunk, columns=result.keys())\n",
    "                chunks.append(chunk_df)\n",
    "                total_rows += len(chunk_df)\n",
    "                chunk_num += 1\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                rows_per_sec = total_rows / elapsed if elapsed > 0 else 0\n",
    "                \n",
    "                print(f\"Fetched {total_rows} rows so far... ({rows_per_sec:.2f} rows/sec, chunk {chunk_num})\")\n",
    "                \n",
    "                # Write chunk to disk to avoid memory issues\n",
    "                if len(chunks) >= 5:  # After accumulating 5 chunks\n",
    "                    print(f\"Writing chunks to {filename}...\")\n",
    "                    combined = pd.concat(chunks, ignore_index=True)\n",
    "                    if not os.path.exists(filename):\n",
    "                        combined.to_csv(filename, index=False)\n",
    "                    else:\n",
    "                        combined.to_csv(filename, mode='a', header=False, index=False)\n",
    "                    chunks = []  # Clear the chunks from memory\n",
    "                    print(f\"Wrote {len(combined)} rows to file, continuing fetch...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        # Process any remaining chunks\n",
    "        if chunks:\n",
    "            print(f\"Writing final chunks to {filename}...\")\n",
    "            combined = pd.concat(chunks, ignore_index=True)\n",
    "            if not os.path.exists(filename):\n",
    "                combined.to_csv(filename, index=False)\n",
    "            else:\n",
    "                combined.to_csv(filename, mode='a', header=False, index=False)\n",
    "                \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Fetch completed in {total_time:.2f} seconds\")\n",
    "        return total_rows\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_in_chunks: {str(e)}\")\n",
    "        return 0\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "# Fetch and save darkpool trades in chunks\n",
    "print(\"\\nFetching and saving darkpool trades...\")\n",
    "total_darkpool_rows = fetch_in_chunks(darkpool_query, engine, darkpool_filename)\n",
    "print(f\"Completed saving {total_darkpool_rows} darkpool trades to {darkpool_filename}\")\n",
    "\n",
    "# Fetch and save options flow data in chunks\n",
    "print(\"\\nFetching and saving options flow data...\")\n",
    "total_options_rows = fetch_in_chunks(options_query, engine, options_filename)\n",
    "print(f\"Completed saving {total_options_rows} option flows to {options_filename}\")\n",
    "\n",
    "print(\"\\nFull data fetch complete.\")\n",
    "print(\"\\nGenerating summary statistics...\")\n",
    "\n",
    "# Process and summarize data from the saved files\n",
    "try:\n",
    "    # Load a sample of the data to generate summary statistics\n",
    "    trades_sample = pd.read_csv(darkpool_filename, nrows=100000)\n",
    "    options_sample = pd.read_csv(options_filename, nrows=100000)\n",
    "    \n",
    "    # Process darkpool trades\n",
    "    trades_sample['executed_at'] = pd.to_datetime(trades_sample['executed_at'])\n",
    "    if 'collection_time' in trades_sample.columns:\n",
    "        trades_sample['collection_time'] = pd.to_datetime(trades_sample['collection_time'])\n",
    "    trades_sample['trade_hour'] = pd.to_datetime(trades_sample['trade_hour'])\n",
    "    \n",
    "    # Process options flow\n",
    "    options_sample['collected_at'] = pd.to_datetime(options_sample['collected_at'])\n",
    "    if 'created_at' in options_sample.columns:\n",
    "        options_sample['created_at'] = pd.to_datetime(options_sample['created_at'])\n",
    "    if 'expiry' in options_sample.columns:\n",
    "        options_sample['expiry'] = pd.to_datetime(options_sample['expiry'])\n",
    "    options_sample['flow_hour'] = pd.to_datetime(options_sample['flow_hour'])\n",
    "    \n",
    "    # Print darkpool trade summary (from sample)\n",
    "    print(\"\\nDarkpool Trade sample summary by symbol (first 100k rows):\")\n",
    "    print(trades_sample.groupby('symbol').agg({\n",
    "        'size': ['count', 'sum', 'mean'],\n",
    "        'premium': ['mean', 'max'],\n",
    "        'price_impact_pct': 'mean'\n",
    "    }).round(2))\n",
    "    \n",
    "    # Print options flow summary (from sample)\n",
    "    print(\"\\nOptions Flow sample summary by symbol (first 100k rows):\")\n",
    "    print(options_sample.groupby('symbol').agg({\n",
    "        'premium': ['count', 'sum', 'mean', 'max'],\n",
    "        'contract_size': ['sum', 'mean'],\n",
    "        'iv_rank': 'mean'\n",
    "    }).round(2))\n",
    "    \n",
    "    # Print date ranges for samples\n",
    "    print(\"\\nDate ranges (from samples):\")\n",
    "    print(\"Darkpool Trades:\")\n",
    "    print(f\"Earliest trade in sample: {trades_sample['executed_at'].min()}\")\n",
    "    print(f\"Latest trade in sample: {trades_sample['executed_at'].max()}\")\n",
    "    print(f\"Total trades fetched: {total_darkpool_rows}\")\n",
    "    \n",
    "    print(\"\\nOptions Flow:\")\n",
    "    print(f\"Earliest flow in sample: {options_sample['collected_at'].min()}\")\n",
    "    print(f\"Latest flow in sample: {options_sample['collected_at'].max()}\")\n",
    "    print(f\"Total flows fetched: {total_options_rows}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating summary statistics: {str(e)}\")\n",
    "    print(\"Data has been saved to files, but summary statistics could not be generated.\")\n",
    "\n",
    "print(\"\\nFull data is available in:\")\n",
    "print(f\"- {darkpool_filename}\")\n",
    "print(f\"- {options_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d12078-df8f-4de5-b03c-756c5b361ef2",
   "metadata": {},
   "source": [
    "# Get Trades for Last 24 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9099609e-cb15-471d-b1a5-de4a5ee720ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (116 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/iversen/Work/veveve/vvv-invest/venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/iversen/Work/veveve/vvv-invest/venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/iversen/Work/veveve/vvv-invest/venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/iversen/Work/veveve/vvv-invest/venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-2.2.5-cp313-cp313-macosx_11_0_arm64.whl (14.2 MB)\n",
      "Installing collected packages: numpy, pandas\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pandas]━━━━\u001b[0m \u001b[32m1/2\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.5 pandas-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a723e1a6-3e0c-4252-8294-28fcd3b1335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dark pool trades from last 24 hours...\n",
      "Fetching options flow data from last 24 hours...\n",
      "\n",
      "Saved 8519 trades to data/darkpool_trades_24h_20250509_215337.csv\n",
      "Saved 0 option flows to data/options_flow_24h_20250509_215337.csv\n",
      "\n",
      "Darkpool Trade summary by symbol:\n",
      "        size                         premium             price_impact_pct\n",
      "       count         sum     mean       mean         max             mean\n",
      "symbol                                                                   \n",
      "QQQ      819   3819097.0  4663.12  311620.82  27980000.0              0.0\n",
      "SPY     7700  35682868.0  4634.14  318501.56  59680000.0              0.0\n",
      "\n",
      "Options Flow summary by symbol:\n",
      "Empty DataFrame\n",
      "Columns: [(premium, count), (premium, sum), (premium, mean), (premium, max), (contract_size, sum), (contract_size, mean), (iv_rank, mean)]\n",
      "Index: []\n",
      "\n",
      "Date ranges:\n",
      "Darkpool Trades:\n",
      "Earliest trade: 2025-05-09 13:32:56+00:00\n",
      "Latest trade: 2025-05-09 19:53:00+00:00\n",
      "Total trades: 8519\n",
      "Total volume: 39,501,965\n",
      "\n",
      "Options Flow:\n",
      "Earliest flow: NaT\n",
      "Latest flow: NaT\n",
      "Total flows: 0\n",
      "Total premium: $0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Database connection setup (same as before)\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'defaultdb',\n",
    "    'user': 'doadmin',\n",
    "    'password': 'AVNS_SrG4Bo3B7uCNEPONkE4',\n",
    "    'host': 'vvv-trading-db-do-user-2110609-0.i.db.ondigitalocean.com',\n",
    "    'port': '25060'\n",
    "}\n",
    "\n",
    "DATABASE_URL = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "engine = create_engine(DATABASE_URL, connect_args={'sslmode': 'require'})\n",
    "\n",
    "# Calculate timestamp for 24 hours ago\n",
    "twenty_four_hours_ago = datetime.now() - timedelta(hours=24)\n",
    "\n",
    "# List of symbols to include\n",
    "SYMBOLS = [\"SPY\", \"QQQ\", \"GLD\"]\n",
    "\n",
    "# Query for dark pool trades from last 24 hours for selected symbols\n",
    "darkpool_query = \"\"\"\n",
    "SELECT \n",
    "    t.*,\n",
    "    date_trunc('hour', t.executed_at) as trade_hour,\n",
    "    t.price - t.nbbo_bid as price_impact,\n",
    "    CASE \n",
    "        WHEN t.nbbo_bid != 0 THEN (t.price - t.nbbo_bid) / t.nbbo_bid\n",
    "        ELSE NULL\n",
    "    END as price_impact_pct,\n",
    "    CASE \n",
    "        WHEN t.size >= 10000 THEN 'Block Trade'\n",
    "        WHEN t.premium >= 0.02 THEN 'High Premium'\n",
    "        ELSE 'Regular'\n",
    "    END as trade_type,\n",
    "    count(*) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as trades_per_hour,\n",
    "    sum(t.size) over (partition by t.symbol, date_trunc('hour', t.executed_at)) as volume_per_hour\n",
    "FROM trading.darkpool_trades t\n",
    "WHERE t.executed_at >= %(twenty_four_hours_ago)s\n",
    "  AND t.symbol = ANY(%(symbols)s)\n",
    "ORDER BY t.executed_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# Query for options flow from last 24 hours for selected symbols\n",
    "options_query = \"\"\"\n",
    "SELECT \n",
    "    f.*,\n",
    "    date_trunc('hour', f.collected_at) as flow_hour,\n",
    "    CASE \n",
    "        WHEN f.premium >= 1000000 THEN 'Whale'\n",
    "        WHEN f.premium >= 100000 THEN 'Large'\n",
    "        ELSE 'Regular'\n",
    "    END as flow_size,\n",
    "    count(*) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as flows_per_hour,\n",
    "    sum(f.premium) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as premium_per_hour,\n",
    "    sum(f.contract_size) over (partition by f.symbol, date_trunc('hour', f.collected_at)) as contracts_per_hour\n",
    "FROM trading.options_flow f\n",
    "WHERE f.collected_at >= %(twenty_four_hours_ago)s\n",
    "  AND f.symbol = ANY(%(symbols)s)\n",
    "ORDER BY f.collected_at DESC\n",
    "\"\"\"\n",
    "\n",
    "# Fetch both datasets with the time parameter and symbols\n",
    "print(\"Fetching dark pool trades from last 24 hours...\")\n",
    "trades_df = pd.read_sql_query(\n",
    "    darkpool_query, engine, params={'twenty_four_hours_ago': twenty_four_hours_ago, 'symbols': SYMBOLS}\n",
    ")\n",
    "\n",
    "print(\"Fetching options flow data from last 24 hours...\")\n",
    "options_df = pd.read_sql_query(\n",
    "    options_query, engine, params={'twenty_four_hours_ago': twenty_four_hours_ago, 'symbols': SYMBOLS}\n",
    ")\n",
    "\n",
    "# Process darkpool trades\n",
    "trades_df['executed_at'] = pd.to_datetime(trades_df['executed_at'])\n",
    "trades_df['collection_time'] = pd.to_datetime(trades_df['collection_time'])\n",
    "trades_df['trade_hour'] = pd.to_datetime(trades_df['trade_hour'])\n",
    "\n",
    "# Process options flow\n",
    "options_df['collected_at'] = pd.to_datetime(options_df['collected_at'])\n",
    "options_df['created_at'] = pd.to_datetime(options_df['created_at'])\n",
    "options_df['expiry'] = pd.to_datetime(options_df['expiry'])\n",
    "options_df['flow_hour'] = pd.to_datetime(options_df['flow_hour'])\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Generate filenames with current timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "darkpool_filename = f'data/darkpool_trades_24h_{timestamp}.csv'\n",
    "options_filename = f'data/options_flow_24h_{timestamp}.csv'\n",
    "\n",
    "# Save both datasets\n",
    "trades_df.to_csv(darkpool_filename, index=False)\n",
    "options_df.to_csv(options_filename, index=False)\n",
    "\n",
    "print(f\"\\nSaved {len(trades_df)} trades to {darkpool_filename}\")\n",
    "print(f\"Saved {len(options_df)} option flows to {options_filename}\")\n",
    "\n",
    "# Print darkpool trade summary\n",
    "print(\"\\nDarkpool Trade summary by symbol:\")\n",
    "print(trades_df.groupby('symbol').agg({\n",
    "    'size': ['count', 'sum', 'mean'],\n",
    "    'premium': ['mean', 'max'],\n",
    "    'price_impact_pct': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "# Print options flow summary\n",
    "print(\"\\nOptions Flow summary by symbol:\")\n",
    "print(options_df.groupby('symbol').agg({\n",
    "    'premium': ['count', 'sum', 'mean', 'max'],\n",
    "    'contract_size': ['sum', 'mean'],\n",
    "    'iv_rank': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "# Print date ranges for both datasets\n",
    "print(\"\\nDate ranges:\")\n",
    "print(\"Darkpool Trades:\")\n",
    "print(f\"Earliest trade: {trades_df['executed_at'].min()}\")\n",
    "print(f\"Latest trade: {trades_df['executed_at'].max()}\")\n",
    "print(f\"Total trades: {len(trades_df)}\")\n",
    "print(f\"Total volume: {trades_df['size'].sum():,.0f}\")\n",
    "\n",
    "print(\"\\nOptions Flow:\")\n",
    "print(f\"Earliest flow: {options_df['collected_at'].min()}\")\n",
    "print(f\"Latest flow: {options_df['collected_at'].max()}\")\n",
    "print(f\"Total flows: {len(options_df)}\")\n",
    "print(f\"Total premium: ${options_df['premium'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27ed83-d5c1-41b9-a04b-565f8ccabd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b1ff6-3fd1-405b-aeea-96cae4b70ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
